# CMB: A Comprehensive  Medical Benchmark in Chinese
![CMB](assets/title.png)
<p align="center">
   ğŸ“ƒ <a href="https://arxiv.org/abs/2308.08833" target="_blank">Paper</a> â€¢ ğŸŒ <a href="https://cmedbenchmark.llmzoo.com/#home" target="_blank">Website</a> â€¢ ğŸ¤— <a href="https://huggingface.co/datasets/FreedomIntelligence/CMB" target="_blank">HuggingFace</a>  

## ğŸŒˆ Update

* **[2023.08.01]** ğŸ‰ğŸ‰ğŸ‰ CMB is publishedï¼ğŸ‰ğŸ‰ğŸ‰
* **[2023.08.21]** [Paper](https://arxiv.org/abs/2308.08833) released.



## ğŸŒ Download Data

- (Recommended) Download the [zip file](https://github.com/FreedomIntelligence/CMB/tree/main/data) and unzip:
    ```bash
    git clone "https://github.com/FreedomIntelligence/CMB.git" && cd CMB && unzip "./data/CMB.zip" -d "./data/" && rm "./data/CMB.zip"
    ```
- Or Check out [HuggingFace datasets](https://huggingface.co/datasets/FreedomIntelligence/CMB) to load our data as follows:
  ```python
  from datasets import load_dataset
  # CMB-Exam datasets ï¼ˆmultiple-choice and multiple-answer questionsï¼‰
  exam_datasets = load_dataset('FreedomIntelligence/CMB','exam')
  # CMB-Clin datasets
  clin_datasets = load_dataset('FreedomIntelligence/CMB','clin')
  ```



## ğŸ¥‡ Leaderboard 

Please Check [Leaderboard](https://cmedbenchmark.llmzoo.com/static/leaderboard.html).



## ğŸ¥¸ Dataset intro
![CMB](assets/CMB-2.svg)
### Components

- CMB-Exam: Comprehensive multi-level assessment for medical knowledge
   - Structure: 6 major categories and 28 subcategories, [View Catalog](catalog.md)
   - CMB-test: 400 questions per subcategories, 11200 questions in total
   - CMB-val: 280 questions with solutions and explanations; used as source for CoT and few-shot
   - CMB-train: 269359 questions for medical knowledge injection
    
- CMB-Clin: 74 cases of complex medical inquires 



### CMB-Exam Item 
```json
{
    "exam_type": "åŒ»å¸ˆè€ƒè¯•",
    "exam_class": "æ‰§ä¸šåŒ»å¸ˆ",
    "exam_subject": "å£è…”æ‰§ä¸šåŒ»å¸ˆ",
    "question": "æ‚£è€…ï¼Œç”·æ€§ï¼Œ11å²ã€‚è¿‘2ä¸ªæœˆæ¥æ—¶æœ‰ä½çƒ­ï¼ˆ37ï½38â„ƒï¼‰ï¼Œå…¨èº«æ— æ˜æ˜¾ç—‡çŠ¶ã€‚æŸ¥ä½“æ— æ˜æ˜¾é˜³æ€§ä½“å¾ã€‚Xçº¿æ£€æŸ¥å‘ç°å³è‚ºä¸­éƒ¨æœ‰ä¸€ç›´å¾„çº¦0.8cmç±»åœ†å½¢ç—…ç¶ï¼Œè¾¹ç¼˜ç¨æ¨¡ç³Šï¼Œè‚ºé—¨æ·‹å·´ç»“è‚¿å¤§ã€‚æ­¤ç”·å­©å¯èƒ½æ‚£",
    "answer": "D",
    "question_type": "å•é¡¹é€‰æ‹©é¢˜",
    "option": {
        "A": "å°å¶å‹è‚ºç‚",
        "B": "æµ¸æ¶¦æ€§è‚ºç»“æ ¸",
        "C": "ç»§å‘æ€§è‚ºç»“æ ¸",
        "D": "åŸå‘æ€§è‚ºç»“æ ¸",
        "E": "ç²Ÿç²’å‹è‚ºç»“æ ¸"
    }
},
```
- exam_type: major category
- exam_class: sub-category
- exam_subject: Specific departments or subdivisions of disciplines 
- question_type: *multiple-choice (å•é¡¹é€‰æ‹©é¢˜)* or *multiple-answer (å¤šé¡¹é€‰æ‹©é¢˜)* 

### CMB-Clin Item 
```json
{
    "id": 0,
    "title": "æ¡ˆä¾‹åˆ†æ-è…¹å¤–ç–",
    "description": "ç°ç—…å²\nï¼ˆ1ï¼‰ç—…å²æ‘˜è¦\n     ç—…äººï¼Œç”·ï¼Œ49å²ï¼Œ3å°æ—¶å‰è§£å¤§ä¾¿åå‡ºç°å³ä¸‹è…¹ç–¼ç—›ï¼Œå³ä¸‹è…¹å¯è§¦åŠä¸€åŒ…å—ï¼Œæ—¢å¾€ä½“å¥ã€‚\nï¼ˆ2ï¼‰ä¸»è¯‰\n     å³ä¸‹è…¹ç—›å¹¶è‡ªæ‰ªåŠåŒ…å—3å°æ—¶ã€‚\n\nä½“æ ¼æ£€æŸ¥\nä½“æ¸©ï¼š T 37.8â„ƒï¼ŒP 101æ¬¡ï¼åˆ†ï¼Œå‘¼å¸22æ¬¡/åˆ†ï¼ŒBP 100/60mmHgï¼Œè…¹è½¯ï¼Œæœªè§èƒƒè‚ å‹è •åŠ¨æ³¢ï¼Œè‚è„¾è‚‹ä¸‹æœªåŠï¼Œäºå³ä¾§è…¹è‚¡æ²ŸåŒºå¯æ‰ªåŠä¸€åœ†å½¢è‚¿å—ï¼Œçº¦4cmÃ—4cmå¤§å°ï¼Œæœ‰å‹ç—›ã€ç•Œæ¬ æ¸…ï¼Œä¸”è‚¿å—ä½äºè…¹è‚¡æ²ŸéŸ§å¸¦ä¸Šå†…æ–¹ã€‚\n\nè¾…åŠ©æ£€æŸ¥\nï¼ˆ1ï¼‰å®éªŒå®¤æ£€æŸ¥\n     è¡€å¸¸è§„ï¼šWBC 5.0Ã—109ï¼Lï¼ŒN 78ï¼…ã€‚\n     å°¿å¸¸è§„æ­£å¸¸ã€‚\nï¼ˆ2ï¼‰å¤šæ™®å‹’è¶…å£°æ£€æŸ¥\n     æ²¿è…¹è‚¡æ²Ÿçºµåˆ‡å¯è§ä¸€å¤šå±‚åˆ†å¸ƒçš„æ··åˆå›å£°åŒºï¼Œå®½çª„ä¸ç­‰ï¼Œè¿œç«¯è†¨å¤§ï¼Œè¾¹ç•Œæ•´é½ï¼Œé•¿çº¦4ï½5cmã€‚\nï¼ˆ3ï¼‰è…¹éƒ¨Xçº¿æ£€æŸ¥\n     å¯è§é˜¶æ¢¯çŠ¶æ¶²æ°”å¹³ã€‚",
    "QA_pairs": [
        {
            "question": "ç®€è¿°è¯¥ç—…äººçš„è¯Šæ–­åŠè¯Šæ–­ä¾æ®ã€‚",
            "solution": "è¯Šæ–­ï¼šåµŒé¡¿æ€§è…¹è‚¡æ²Ÿæ–œç–åˆå¹¶è‚ æ¢—é˜»ã€‚\nè¯Šæ–­ä¾æ®ï¼š\nâ‘ å³ä¸‹è…¹ç—›å¹¶è‡ªæ‰ªåŠåŒ…å—3å°æ—¶ï¼›\nâ‘¡æœ‰è…¹èƒ€ã€å‘•åï¼Œç±»ä¼¼è‚ æ¢—é˜»è¡¨ç°ï¼›è…¹éƒ¨å¹³ç‰‡å¯è§é˜¶æ¢¯çŠ¶æ¶²å¹³ï¼Œè€ƒè™‘è‚ æ¢—é˜»å¯èƒ½ï¼›è…¹éƒ¨Bè¶…è€ƒè™‘ï¼Œ\nè…¹éƒ¨åŒ…å—å†…å¯èƒ½ä¸ºè‚ ç®¡å¯èƒ½ï¼›\nâ‘¢æœ‰è½»åº¦æ¯’æ€§ååº”æˆ–æ˜¯ä¸­æ¯’ååº”ï¼Œå¦‚ T 37.8â„ƒï¼ŒP 101æ¬¡ï¼åˆ†ï¼Œç™½ç»†èƒä¸­æ€§åˆ†ç±»78ï¼…ï¼›\nâ‘£è…¹è‚¡æ²ŸåŒºåŒ…å—ä½äºè…¹è‚¡æ²ŸéŸ§å¸¦ä¸Šå†…æ–¹ã€‚"
        },
        {
            "question": "ç®€è¿°è¯¥ç—…äººçš„é‰´åˆ«è¯Šæ–­ã€‚",
            "solution": "ï¼ˆ1ï¼‰ç¾ä¸¸é˜è†œç§¯æ¶²ï¼šé˜è†œç§¯æ¶²æ‰€å‘ˆç°çš„è‚¿å—å®Œå…¨å±€é™åœ¨é˜´å›Šå†…ï¼Œå…¶ä¸Šç•Œå¯ä»¥æ¸…æ¥šåœ°æ‘¸åˆ°ï¼›ç”¨é€å…‰è¯•éªŒæ£€æŸ¥è‚¿å—ï¼Œé˜è†œç§¯æ¶²å¤šä¸ºé€å…‰ï¼ˆé˜³æ€§ï¼‰ï¼Œè€Œç–å—åˆ™ä¸èƒ½é€å…‰ã€‚\nï¼ˆ2ï¼‰äº¤é€šæ€§é˜è†œç§¯æ¶²ï¼šè‚¿å—çš„å¤–å½¢ä¸ç¾ä¸¸é˜è†œç§¯æ¶²ç›¸ä¼¼ã€‚äºæ¯æ—¥èµ·åºŠåæˆ–ç«™ç«‹æ´»åŠ¨æ—¶è‚¿å—ç¼“æ…¢åœ°å‡ºç°å¹¶å¢å¤§ã€‚å¹³å§æˆ–ç¡è§‰åè‚¿å—é€æ¸ç¼©å°ï¼ŒæŒ¤å‹è‚¿å—ï¼Œå…¶ä½“ç§¯ä¹Ÿå¯é€æ¸ç¼©å°ã€‚é€å…‰è¯•éªŒä¸ºé˜³æ€§ã€‚\nï¼ˆ3ï¼‰ç²¾ç´¢é˜è†œç§¯æ¶²ï¼šè‚¿å—è¾ƒå°ï¼Œåœ¨è…¹è‚¡æ²Ÿç®¡å†…ï¼Œç‰µæ‹‰åŒä¾§ç¾ä¸¸å¯è§è‚¿å—ç§»åŠ¨ã€‚\nï¼ˆ4ï¼‰éšç¾ï¼šè…¹è‚¡æ²Ÿç®¡å†…ä¸‹é™ä¸å…¨çš„ç¾ä¸¸å¯è¢«è¯¯è¯Šä¸ºæ–œç–æˆ–ç²¾ç´¢é˜è†œç§¯æ¶²ã€‚éšç¾è‚¿å—è¾ƒå°ï¼ŒæŒ¤å‹æ—¶å¯å‡ºç°ç‰¹æœ‰çš„èƒ€ç—›æ„Ÿè§‰ã€‚å¦‚æ‚£ä¾§é˜´å›Šå†…ç¾ä¸¸ç¼ºå¦‚ï¼Œåˆ™è¯Šæ–­æ›´ä¸ºæ˜ç¡®ã€‚\nï¼ˆ5ï¼‰æ€¥æ€§è‚ æ¢—é˜»ï¼šè‚ ç®¡è¢«åµŒé¡¿çš„ç–å¯ä¼´å‘æ€¥æ€§è‚ æ¢—é˜»ï¼Œä½†ä¸åº”ä»…æ»¡è¶³äºè‚ æ¢—é˜»çš„è¯Šæ–­è€Œå¿½ç•¥ç–çš„å­˜åœ¨ï¼›å°¤å…¶æ˜¯ç—…äººæ¯”è¾ƒè‚¥èƒ–æˆ–ç–å—è¾ƒå°æ—¶ï¼Œæ›´æ˜“å‘ç”Ÿè¿™ç±»é—®é¢˜è€Œå¯¼è‡´æ²»ç–—ä¸Šçš„é”™è¯¯ã€‚\nï¼ˆ6ï¼‰æ­¤å¤–ï¼Œè…¹è‚¡æ²ŸåŒºè‚¿å—è¿˜åº”ä¸ä»¥ä¸‹ç–¾ç—…é‰´åˆ«:è‚¿å¤§çš„æ·‹å·´ç»“ã€åŠ¨ï¼ˆé™ï¼‰è„‰ç˜¤ã€è½¯ç»„ç»‡è‚¿ç˜¤ã€è„“è‚¿ã€\nåœ†éŸ§å¸¦å›Šè‚¿ã€å­å®«å†…è†œå¼‚ä½ç—‡ç­‰ã€‚"
        },
        {
            "question": "ç®€è¿°è¯¥ç—…äººçš„æ²»ç–—åŸåˆ™ã€‚",
            "solution": "åµŒé¡¿æ€§ç–åŸåˆ™ä¸Šéœ€è¦ç´§æ€¥æ‰‹æœ¯æ²»ç–—ï¼Œä»¥é˜²æ­¢ç–å†…å®¹ç‰©åæ­»å¹¶è§£é™¤ä¼´å‘çš„è‚ æ¢—é˜»ã€‚æœ¯å‰åº”åšå¥½å¿…è¦çš„å‡†å¤‡ï¼Œå¦‚æœ‰è„±æ°´å’Œç”µè§£è´¨ç´Šä¹±ï¼Œåº”è¿…é€Ÿè¡¥æ¶²åŠ ä»¥çº æ­£ã€‚æ‰‹æœ¯çš„å…³é”®åœ¨äºæ­£ç¡®åˆ¤æ–­ç–å†…å®¹ç‰©çš„æ´»åŠ›ï¼Œç„¶åæ ¹æ®ç—…æƒ…ç¡®å®šå¤„ç†æ–¹æ³•ã€‚åœ¨æ‰©å¼ æˆ–åˆ‡å¼€ç–ç¯ã€è§£é™¤ç–ç¯å‹è¿«çš„å‰æä¸‹ï¼Œå‡¡è‚ ç®¡å‘ˆç´«é»‘è‰²ï¼Œå¤±å»å…‰æ³½å’Œå¼¹æ€§ï¼Œåˆºæ¿€åæ— è •åŠ¨å’Œç›¸åº”è‚ ç³»è†œå†…æ— åŠ¨è„‰æåŠ¨è€…ï¼Œå³å¯åˆ¤å®šä¸ºè‚ åæ­»ã€‚å¦‚è‚ ç®¡å°šæœªåæ­»ï¼Œåˆ™å¯å°†å…¶é€å›è…¹è…”ï¼ŒæŒ‰ä¸€èˆ¬æ˜“å¤æ€§ç–å¤„ç†ï¼Œå³è¡Œç–å›Šé«˜ä½ç»“æ‰+ç–ä¿®è¡¥æœ¯ã€‚å¦‚è‚ ç®¡ç¡®å·²åæ­»æˆ–ä¸€æ—¶ä¸èƒ½è‚¯å®šè‚ ç®¡æ˜¯å¦å·²å¤±å»æ´»åŠ›æ—¶ï¼Œåˆ™åº”åœ¨ç—…äººå…¨èº«æƒ…å†µå…è®¸çš„å‰æä¸‹ï¼Œåˆ‡é™¤è¯¥æ®µè‚ ç®¡å¹¶è¿›è¡Œä¸€æœŸå»åˆã€‚å‡¡æ–½è¡Œè‚ åˆ‡é™¤å»åˆæœ¯çš„ç—…äººï¼Œå› æ‰‹æœ¯åŒºæ±¡æŸ“ï¼Œåœ¨é«˜ä½ç»“æ‰ç–å›Šåï¼Œä¸€èˆ¬ä¸å®œä½œç–ä¿®è¡¥æœ¯ï¼Œä»¥å…å› æ„ŸæŸ“è€Œè‡´ä¿®è¡¥å¤±è´¥ã€‚"
        }
    ]
},
```
- title: name of disease
- description: information of patient
- QA_pairs: a series of questions and their solutions based on the description





## â„¹ï¸ How to evaluate and submit

### Modify model configuration file
<details><summary>Click to expand</summary>

`configs/model_config.yaml`ï¼š
```
my_model:
    model_id: 'my_model'
    load:
        # # HuggingFace model weights
        config_dir: "path/to/full/model"

        # # load with Peft
        # llama_dir: "path/to/base"
        # lora_dir: "path/to/lora"

        device: 'cuda'          # only support cuda
        precision: 'fp16'       # 

    # supports all parameters in transformers.GenerationConfig
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         
```
</details>

### Modify model worker
<details><summary>Click to expand</summary>

In `workers/mymodel.py`:
1. load model and tokenizer to cpu
   ```
   def load_model_and_tokenizer(self, load_config):
        '''
        Params: 
            load_config: the `load` key in `configs/model_config.yaml`
        Returns:
            model, tokenizer: both on cpu
        '''
        hf_model_config = {"pretrained_model_name_or_path": load_config['config_dir'],'trust_remote_code': True, 'low_cpu_mem_usage': True}
        hf_tokenizer_config = {"pretrained_model_name_or_path": load_config['config_dir'], 'padding_side': 'left', 'trust_remote_code': True}
        precision = load_config.get('precision', 'fp16')
        device = load_config.get('device', 'cuda')

        if precision == 'fp16':
            hf_model_config.update({"torch_dtype": torch.float16})

        model = AutoModelForCausalLM.from_pretrained(**hf_model_config)
        tokenizer = AutoTokenizer.from_pretrained(**hf_tokenizer_config)

        model.eval()
        return model, tokenizer # cpu
   ```

2. system prompt
    ```
    @property
    def system_prompt(self):
        '''
        The prompt that is prepended to every input.
        '''
        return "ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚"
    ```

3. instruction template
    ```
    @property
    def instruction_template(self):
        '''
        The template for instruction input. An '{instruction}' placeholder must be contained.
        '''
        return self.system_prompt + 'é—®ï¼š{instruction}\nç­”ï¼š'
    ```

4. instruction template with fewshot examples
    ```
    @property
    def instruction_template_with_fewshot(self,):
        '''
        The template for instruction input. There must be an '{instruction}' placeholder in this template.
        '''
        return self.system_prompt + '{fewshot_examples}é—®ï¼š{instruction}\nç­”ï¼š'  # å¿…é¡»å¸¦æœ‰ {instruction} å’Œ {fewshot_examples} çš„placeholder
    ```
    
5. template for each fewshot example
    ```
    @property
    def fewshot_template(self):
        '''
        The template for each fewshot example. Each fewshot example is concatenated and put in the `{fewshot_examples}` placeholder above.
        There must be a `{user}` and `{gpt}` placeholder in this template.
        '''
        return "é—®ï¼š{user}\nç­”ï¼š{gpt}\n" # å¿…é¡»å¸¦æœ‰ {user} å’Œ {gpt} çš„placeholder
    ```
</details>



### Generate fewshot examples (required if using fewshot)
<details><summary>Click to expand</summary>

Modify `generate_fewshot.sh`:
```bash
model_id="baichuan-13b-chat"
n_shot=3

test_path=data/CMB-Exam/CMB-test/CMB-test-choice-question-merge.json 
val_path=data/CMB-Exam/CMB-val/CMB-val-merge.json
output_dir=data/fewshot
python ./src/generate_fewshot.py \
--use_cot \                     # whether to use CoT template
--n_shot=$n_shot \
--model_id=$model_id \
--output_dir=$output_dir  \
--val_path=$val_path \
--test_path=$test_path 
```

and run:
```bash
bash generate_fewshot.sh

```

</details>


### Modify the main script 
<details><summary>Click to expand</summary>

`generate_answers.sh`:
```
# # input file path
# data_path='data/CMB-Exam/CMB-test/CMB-test-choice-question-merge.json'   
# data_path='data/CMB-Clin/CMB-Clin-qa.json'                            

task_name='Zero-test-cot'   
port_id=27272

model_id="my_model"                                                      # the same as in `configs/model_config.yaml` 

accelerate launch \
    --gpu_ids='all' \                                                   
    --main_process_port 12345 \                                      
    --config_file ./configs/accelerate_config.yaml  \                   # /path/to/accelerate_config
    ./src/generate_answers.py \                                         # main program
    --model_id=$model_id \                                              # model id
    --use_cot \                                                         # whether to use CoT template   
    --use_fewshot \                                                     # whether to use fewshot
    --batch_size 3  \                                                                                   
    --input_path=$test_data_path \                                      # input path
    --output_path=./result/${task_name}/${model_id}/answers.json \      # output path
    --model_config_path="./configs/model_config.yaml"                   # /path/to/model_config
```
</details>




### Run evaluation
<details><summary>Click to expand</summary>

Step 1: generate answers
```
bash generate_answers.sh
```

Step 2: score your answers

Submit your output in **Step 1** to cmedbenchmark@163.com. You will be notified via email once we score your answers. We will update results to the leaderboard **only if you authorize us to do so**. Before that, your scores will be kept confidential.
</details>


<!-- ## âœ…  CMBè¯„æµ‹ç»†èŠ‚
Generateå‚æ•°: ä¸ºäº†å‡å°‘æ–¹å·®ï¼Œä¸€è‡´å°†Sampleè®¾ç½®ä¸ºFalseè¿›è¡ŒGreedy Decoding -->



## Tricks to improve performance
### Try out different decoding strategies
You can configure hyperparameters for generation in `./configs/model_config.yaml`. We find that a lower temperature often gives higher performance for most models. However, the effects of other hyperparams are not clear.


### Modify answer matching strategy
You can modify the `match_choice()` function in `src/utils.py`. The output patterns of different models vary, which makes it hard for us to consider all cases for all models using a single regular expression. If you find a better matching strategy for those evaluated models in our paper, please submit your results for updates.


## Prompt format
### CMB-Exam Prompt
[CMB-Exam Item](#cmb-exam-item)
#### Answer-only Prompt
```
{System_prompt}

<{Role_1}>ï¼šä»¥ä¸‹æ˜¯ä¸­å›½{exam_type}ä¸­{exam_class}è€ƒè¯•çš„ä¸€é“{question_type}ï¼Œä¸éœ€è¦åšä»»ä½•åˆ†æå’Œè§£é‡Šï¼Œç›´æ¥è¾“å‡ºç­”æ¡ˆé€‰é¡¹ã€‚ã€‚
{é¢˜ç›®}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
...
<{Role_2}>ï¼šA

[n-shot demo, n is 0 for the zero-shot case]

<{Role_1}>ï¼šä»¥ä¸‹æ˜¯ä¸­å›½{exam_type}ä¸­{exam_class}è€ƒè¯•çš„ä¸€é“{question_type}ï¼Œä¸éœ€è¦åšä»»ä½•åˆ†æå’Œè§£é‡Šï¼Œç›´æ¥è¾“å‡ºç­”æ¡ˆé€‰é¡¹ã€‚
{é¢˜ç›®}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
...
<{Role_2}>ï¼š
```
#### Chain-of-thought Prompt

```
{System_prompt}

<{Role_1}>ï¼šä»¥ä¸‹æ˜¯ä¸­å›½{exam_type}ä¸­{exam_class}è€ƒè¯•çš„ä¸€é“{question_type}ï¼Œè¯·åˆ†ææ¯ä¸ªé€‰é¡¹ï¼Œå¹¶æœ€åç»™å‡ºç­”æ¡ˆã€‚
{é¢˜ç›®}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
...
<{Role_2}>ï¼š.......æ‰€ä»¥ç­”æ¡ˆæ˜¯A

[n-shot demo, n is 0 for the zero-shot case]

<{Role_1}>ï¼šä»¥ä¸‹æ˜¯ä¸­å›½{exam_type}ä¸­{exam_class}è€ƒè¯•çš„ä¸€é“{question_type}ï¼Œè¯·åˆ†ææ¯ä¸ªé€‰é¡¹ï¼Œå¹¶æœ€åç»™å‡ºç­”æ¡ˆã€‚
{é¢˜ç›®}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
...
<{Role_2}>ï¼š
```

### CMB-Clin Prompt
[CMB-Clin Item](#cmb-clin-item) 
```
{System_prompt}

<{Role_1}>ï¼šä»¥ä¸‹æ˜¯ä¸€ä½ç—…äººçš„ç—…ä¾‹ï¼š
{description}
{QA_pairs[0]['question']}
<{Role_2}>ï¼š..........
[n-question based on the len(QA_pairs)]
```


<!-- ## Limitations
1. CMB-Clin is converted to multi-turn conversation
2. ç­”æ¡ˆæå–æ–¹å¼å¯èƒ½ä¸å¤Ÿå®Œå–„, è¯¦è§[ä»£ç ](https://github.com/FreedomIntelligence/CMB/blob/main/src/utils.py#L36)ã€‚ -->

## To do List
1. The automatic evaluation function of the official website


##  Citation
Please use the following citation if you intend to use our dataset for training or evaluation:


```
@misc{wang2023cmb,
      title={CMB: A Comprehensive Medical Benchmark in Chinese}, 
      author={Xidong Wang and Guiming Hardy Chen and Dingjie Song and Zhiyi Zhang and Zhihong Chen and Qingying Xiao and Feng Jiang and Jianquan Li and Xiang Wan and Benyou Wang and Haizhou Li},
      year={2023},
      eprint={2308.08833},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

```
@misc{cmedbenchmark,
  title={CMB: Chinese Medical Benchmark},
  author={Xidong Wang*, Guiming Hardy Chen*, Dingjie Song*, Zhiyi Zhang*, Qingying Xiao, Xiangbo Wu, Feng Jiang, Jianquan Li, Benyou Wang},
  note={Xidong Wang, Guiming Hardy Chen, Dingjie Song, and Zhiyi Zhang contributed equally to this github repo.},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/FreedomIntelligence/CMB}},
}
```


## Acknowledgement 
- We thank [Shenzhen Research Institute of Big Data](http://www.sribd.cn/) for their enormous support for this project.

- We thank the following doctors for participating in the human evaluation of CMB-Clin:
    
    - æ—å£«å†› (é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰é™„å±ç¬¬äºŒåŒ»é™¢)
    - å¸¸æ²³
    - è®¸æ™“çˆ½
