
huatuo:
    model_id: "huatuo"
    load:
        config_dir: "./models/latest_tfmr"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         


huatuo-chat: # the public version on HF 
    model_id: "huatuo-chat"
    load:
        config_dir: "./models/download_huatuogpt_7b"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         



doctorglm:
    model_id: 'doctorglm'
    load:
        config_dir: ./models/chatglm-6b
        prefix_config_dir: ./models/DoctorGLM/Doctor_GLM/ckpt/ptuningv2/ptuningv2/pytorch_model.bin
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1      
        do_sample: False         



bentsao:
    model_id: "bentsao"
    load:
        llama_dir: "./models/llama_hf_7b" # huggingface llama
        lora_dir: "./models/Huatuo-Llama-Med-Chinese/lora-llama-med"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False     
        # temperature: 0.1
        # top_p: 0.75
        # top_k: 40
        # num_beams: 4
        # min_new_tokens: 20
        # max_new_tokens: 256  


bianque-v2:
    model_id: 'bianque-v2'
    load:
        config_dir: "./models/bianque-2"
        device: 'cuda'          # ['cuda', 'cpu', 'mps']
        precision: 'fp16'
    generation_config: # pass any huggingface generation configs here
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         



chatglm-med:
    model_id: 'chatglm-med'
    load:
        config_dir: "./models/ChatGLM-Med"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         

chatglm3_6b:
    model_id: 'chatglm3_6b'
    load:
        config_dir: "./models/chatglm3-6b"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False   

qizhen-cama-13b: # qizhen-13b
    model_id: 'qizhen-cama-13b'
    load:
        llama_dir: "./models/zhixi-13b"
        lora_dir: "./models/qizhen-lora"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         



chatmed-consult:
    model_id: 'chatmed-consult'
    load:
        llama_dir: "./models/chinese-llama-alpaca-plus-lora-7b"
        lora_dir: "./models/chatmed-consult"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         



medicalgpt:
    model_id: 'medicalgpt'
    load:
        config_dir: "./models/medicalgpt"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False         


baichuan-13b-chat:
    model_id: 'baichuan-13b-chat'
    load:
        config_dir: "./models/baichuan-13b-chat"
        device: 'cuda'        
        precision: 'fp16'
    generation_config: 
        max_new_tokens: 512     
        min_new_tokens: 1          
        do_sample: False